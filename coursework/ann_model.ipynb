{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from pylab import *\n",
    "\n",
    "%matplotlib inline\n",
    "from scipy.signal import welch\n",
    "from scipy.integrate import simps\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = sio.loadmat('data\\WLDataCW.mat')\n",
    "\n",
    "datafile = 'data\\WLDataCW.mat'\n",
    "data_arr = sio.loadmat(datafile)\n",
    "x_ = data_arr['data']\n",
    "y_ = data_arr['label'].reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into training and test set to contain balanced smaples of Workload 0 and 1 <br>\n",
    "K Cross Validation will be performed further on the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train Set : 288\n",
      "Length of Test Set : 72\n"
     ]
    }
   ],
   "source": [
    "low_idx = np.argwhere(y_==0).reshape(-1)\n",
    "medium_idx = np.argwhere(y_==1).reshape(-1)\n",
    "\n",
    "indices = np.random.permutation(low_idx)\n",
    "\n",
    "training_idx , test_idx = indices[:144] , indices[144:]\n",
    "\n",
    "indices = np.random.permutation(medium_idx)\n",
    "training_idx = np.append(training_idx ,indices[:144] )\n",
    "test_idx = np.append(test_idx ,indices[144:])\n",
    "\n",
    "print(\"Length of Train Set :\" ,len(training_idx))\n",
    "print(\"Length of Test Set :\" ,len(test_idx))\n",
    "\n",
    "\n",
    "x_train = x_[:,:,training_idx]\n",
    "y_train = y_[training_idx]\n",
    "\n",
    "x_test = x_[:,:,test_idx]\n",
    "y_test = y_[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Dataset class to load and tranform the EEG signals for Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Dataset class to tranform the input and output of Training Data and Labels\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_arr,y_arr, transform=None, target_transform=None):\n",
    "        self.data = x_arr\n",
    "        self.labels = y_arr.reshape(-1)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.sf = 256\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        arr_list = []\n",
    "        \n",
    "        for i in  range(self.data.shape[0]):\n",
    "            arr = self.bandpower(self.data[i,:,idx],self.sf,None,True)\n",
    "            arr_list.append(arr)\n",
    "        arr = np.array(arr_list).reshape(-1)\n",
    "        arr = arr.astype(np.float32)\n",
    "        label = self.labels[idx].astype(np.float32)\n",
    "        \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return arr, label\n",
    "    \n",
    "    \n",
    "    # The bandpower calculation is inspired by Welch Method described here : https://raphaelvallat.com/bandpower.html\n",
    "    def bandpower(self,data, sf,window_sec=None, relative=False):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : 1d-array\n",
    "            EEG data from a single electrode\n",
    "        sf : float\n",
    "            Sampling frequency\n",
    "        window_sec : float\n",
    "            Length of each window\n",
    "            Default Value, window_sec = (1 / min(band)) * 2\n",
    "        relative : boolean\n",
    "            Relative Band Power(True) or Absolute (False)\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        bp_arr : 1-d array\n",
    "            Array of Absolute and Relative Band Power for Given Frequency Bands - Delta , Theta , Alpha , Beta , Gamma\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        eeg_bands = {'Delta': (1, 4),\n",
    "                    'Theta': (4, 8),\n",
    "                    'Alpha': (8, 12),\n",
    "                    'Beta': (12, 30),\n",
    "                    'Gamma': (30, 120)\n",
    "                    }\n",
    "        \n",
    "        bp_arr = []\n",
    "        for band in ['Delta','Theta','Alpha','Beta','Gamma']:\n",
    "            low, high = eeg_bands[band][0],eeg_bands[band][1]\n",
    "            \n",
    "            # Define window length\n",
    "            if window_sec is not None:\n",
    "                nseg = window_sec * sf\n",
    "            else:\n",
    "                nseg = (2 / low) * sf\n",
    "            if nseg > sf:\n",
    "                nseg = sf\n",
    "            # Compute the modified periodogram (Welch)\n",
    "            freqs, psd = welch(data, sf, nperseg = nseg)\n",
    "\n",
    "            # Frequency resolution\n",
    "            freq_res = freqs[1] - freqs[0]\n",
    "\n",
    "            # Find closest indices of band in frequency vector\n",
    "            idx_band = np.logical_and(freqs >= low, freqs <= high)\n",
    "\n",
    "            # Integral approximation of the spectrum using Simpson's rule.\n",
    "            bp = simps(psd[idx_band], dx=freq_res)\n",
    "            \n",
    "            # relative to the overal bandpower of signal\n",
    "            if relative:\n",
    "                bp /= simps(psd, dx=freq_res)\n",
    "            bp_arr.append(bp)\n",
    "        return np.array(bp_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification,self).__init__()\n",
    "        self.layer_1 = nn.Linear(310,64)\n",
    "        self.layer_2 = nn.Linear(64,32)\n",
    "        self.layer_out = nn.Linear(32,1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(32)\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        x = self.relu(self.dropout(self.layer_1(inputs)))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.layer_out(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = BinaryClassification()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop : Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    train_loss , correct = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        # print(X)\n",
    "        # print(X.shape)\n",
    "        pred = model(X)\n",
    "        y = y.unsqueeze(1)\n",
    "        loss = loss_fn(pred,y )\n",
    "        # print(loss)\n",
    "        train_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        correct += (torch.round(pred) == y).type(torch.float).sum().item()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    train_loss /= num_batches\n",
    "    correct /= size\n",
    "    train_losses.append((train_loss))\n",
    "    train_acc.append(100*correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            y = y.unsqueeze(1)\n",
    "            \n",
    "            \n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (torch.round(pred) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    val_losses.append((test_loss))\n",
    "    val_acc.append(100*correct)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 20\n",
    "batch_size = 10\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------fold no---------0----------------------\n",
      "Epoch 1\n",
      "----------------------------\n",
      "loss: 0.743601  [    0/  192]\n",
      "loss: 0.696875  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 0.693161 \n",
      "\n",
      "Epoch 2\n",
      "----------------------------\n",
      "loss: 0.754974  [    0/  192]\n",
      "loss: 0.766184  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.664469 \n",
      "\n",
      "Epoch 3\n",
      "----------------------------\n",
      "loss: 0.535604  [    0/  192]\n",
      "loss: 0.587863  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.615284 \n",
      "\n",
      "Epoch 4\n",
      "----------------------------\n",
      "loss: 0.508628  [    0/  192]\n",
      "loss: 0.658119  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.580257 \n",
      "\n",
      "Epoch 5\n",
      "----------------------------\n",
      "loss: 0.528684  [    0/  192]\n",
      "loss: 0.482886  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.533410 \n",
      "\n",
      "Epoch 6\n",
      "----------------------------\n",
      "loss: 0.524264  [    0/  192]\n",
      "loss: 0.719513  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.534738 \n",
      "\n",
      "Epoch 7\n",
      "----------------------------\n",
      "loss: 0.381018  [    0/  192]\n",
      "loss: 0.525281  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.497122 \n",
      "\n",
      "Epoch 8\n",
      "----------------------------\n",
      "loss: 0.472932  [    0/  192]\n",
      "loss: 0.452543  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.492804 \n",
      "\n",
      "Epoch 9\n",
      "----------------------------\n",
      "loss: 0.469559  [    0/  192]\n",
      "loss: 0.524883  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.489714 \n",
      "\n",
      "Epoch 10\n",
      "----------------------------\n",
      "loss: 0.528194  [    0/  192]\n",
      "loss: 0.808379  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.474332 \n",
      "\n",
      "Epoch 11\n",
      "----------------------------\n",
      "loss: 0.434107  [    0/  192]\n",
      "loss: 0.356992  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.460381 \n",
      "\n",
      "Epoch 12\n",
      "----------------------------\n",
      "loss: 0.339797  [    0/  192]\n",
      "loss: 0.403791  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.465809 \n",
      "\n",
      "Epoch 13\n",
      "----------------------------\n",
      "loss: 0.332804  [    0/  192]\n",
      "loss: 0.363470  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.459249 \n",
      "\n",
      "Epoch 14\n",
      "----------------------------\n",
      "loss: 0.392480  [    0/  192]\n",
      "loss: 0.313842  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.445971 \n",
      "\n",
      "Epoch 15\n",
      "----------------------------\n",
      "loss: 0.316044  [    0/  192]\n",
      "loss: 0.430136  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.429304 \n",
      "\n",
      "Epoch 16\n",
      "----------------------------\n",
      "loss: 0.419938  [    0/  192]\n",
      "loss: 0.420089  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.426889 \n",
      "\n",
      "Epoch 17\n",
      "----------------------------\n",
      "loss: 0.504561  [    0/  192]\n",
      "loss: 0.459922  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.414859 \n",
      "\n",
      "Epoch 18\n",
      "----------------------------\n",
      "loss: 0.362007  [    0/  192]\n",
      "loss: 0.387924  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.421551 \n",
      "\n",
      "Epoch 19\n",
      "----------------------------\n",
      "loss: 0.683490  [    0/  192]\n",
      "loss: 0.406456  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.397763 \n",
      "\n",
      "Epoch 20\n",
      "----------------------------\n",
      "loss: 0.479180  [    0/  192]\n",
      "loss: 0.367081  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.395412 \n",
      "\n",
      "------------fold no---------1----------------------\n",
      "Epoch 1\n",
      "----------------------------\n",
      "loss: 0.723530  [    0/  192]\n",
      "loss: 0.604890  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 68.8%, Avg loss: 0.618071 \n",
      "\n",
      "Epoch 2\n",
      "----------------------------\n",
      "loss: 0.711146  [    0/  192]\n",
      "loss: 0.557241  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.547024 \n",
      "\n",
      "Epoch 3\n",
      "----------------------------\n",
      "loss: 0.586515  [    0/  192]\n",
      "loss: 0.564662  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.515973 \n",
      "\n",
      "Epoch 4\n",
      "----------------------------\n",
      "loss: 0.631595  [    0/  192]\n",
      "loss: 0.524623  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.488622 \n",
      "\n",
      "Epoch 5\n",
      "----------------------------\n",
      "loss: 0.732893  [    0/  192]\n",
      "loss: 0.458306  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.447365 \n",
      "\n",
      "Epoch 6\n",
      "----------------------------\n",
      "loss: 0.510309  [    0/  192]\n",
      "loss: 0.470721  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.435382 \n",
      "\n",
      "Epoch 7\n",
      "----------------------------\n",
      "loss: 0.415377  [    0/  192]\n",
      "loss: 0.339840  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.485347 \n",
      "\n",
      "Epoch 8\n",
      "----------------------------\n",
      "loss: 0.475158  [    0/  192]\n",
      "loss: 0.418116  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.414921 \n",
      "\n",
      "Epoch 9\n",
      "----------------------------\n",
      "loss: 0.427936  [    0/  192]\n",
      "loss: 0.381076  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.423250 \n",
      "\n",
      "Epoch 10\n",
      "----------------------------\n",
      "loss: 0.405236  [    0/  192]\n",
      "loss: 0.534500  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.394847 \n",
      "\n",
      "Epoch 11\n",
      "----------------------------\n",
      "loss: 0.604075  [    0/  192]\n",
      "loss: 0.307473  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.368729 \n",
      "\n",
      "Epoch 12\n",
      "----------------------------\n",
      "loss: 0.512200  [    0/  192]\n",
      "loss: 0.519999  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.358265 \n",
      "\n",
      "Epoch 13\n",
      "----------------------------\n",
      "loss: 0.685614  [    0/  192]\n",
      "loss: 0.466562  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.364958 \n",
      "\n",
      "Epoch 14\n",
      "----------------------------\n",
      "loss: 0.452490  [    0/  192]\n",
      "loss: 0.417071  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.388885 \n",
      "\n",
      "Epoch 15\n",
      "----------------------------\n",
      "loss: 0.591140  [    0/  192]\n",
      "loss: 0.304232  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.353821 \n",
      "\n",
      "Epoch 16\n",
      "----------------------------\n",
      "loss: 0.479506  [    0/  192]\n",
      "loss: 0.316588  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.528943 \n",
      "\n",
      "Epoch 17\n",
      "----------------------------\n",
      "loss: 0.513417  [    0/  192]\n",
      "loss: 0.509135  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.377648 \n",
      "\n",
      "Epoch 18\n",
      "----------------------------\n",
      "loss: 0.276183  [    0/  192]\n",
      "loss: 0.553570  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.356243 \n",
      "\n",
      "Epoch 19\n",
      "----------------------------\n",
      "loss: 0.236711  [    0/  192]\n",
      "loss: 0.448577  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.345355 \n",
      "\n",
      "Epoch 20\n",
      "----------------------------\n",
      "loss: 0.419949  [    0/  192]\n",
      "loss: 0.304815  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.343829 \n",
      "\n",
      "------------fold no---------2----------------------\n",
      "Epoch 1\n",
      "----------------------------\n",
      "loss: 0.704499  [    0/  192]\n",
      "loss: 0.498634  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 77.1%, Avg loss: 0.613157 \n",
      "\n",
      "Epoch 2\n",
      "----------------------------\n",
      "loss: 0.588135  [    0/  192]\n",
      "loss: 0.675413  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.626955 \n",
      "\n",
      "Epoch 3\n",
      "----------------------------\n",
      "loss: 0.594671  [    0/  192]\n",
      "loss: 0.571883  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.568500 \n",
      "\n",
      "Epoch 4\n",
      "----------------------------\n",
      "loss: 0.608205  [    0/  192]\n",
      "loss: 0.746069  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.623632 \n",
      "\n",
      "Epoch 5\n",
      "----------------------------\n",
      "loss: 0.674996  [    0/  192]\n",
      "loss: 0.662214  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.548854 \n",
      "\n",
      "Epoch 6\n",
      "----------------------------\n",
      "loss: 0.568746  [    0/  192]\n",
      "loss: 0.684218  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.545809 \n",
      "\n",
      "Epoch 7\n",
      "----------------------------\n",
      "loss: 0.585079  [    0/  192]\n",
      "loss: 0.545917  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.522473 \n",
      "\n",
      "Epoch 8\n",
      "----------------------------\n",
      "loss: 0.530173  [    0/  192]\n",
      "loss: 0.693905  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.519154 \n",
      "\n",
      "Epoch 9\n",
      "----------------------------\n",
      "loss: 0.550251  [    0/  192]\n",
      "loss: 0.577152  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.499851 \n",
      "\n",
      "Epoch 10\n",
      "----------------------------\n",
      "loss: 0.532070  [    0/  192]\n",
      "loss: 0.472624  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.492314 \n",
      "\n",
      "Epoch 11\n",
      "----------------------------\n",
      "loss: 0.441890  [    0/  192]\n",
      "loss: 0.363154  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.493775 \n",
      "\n",
      "Epoch 12\n",
      "----------------------------\n",
      "loss: 0.471938  [    0/  192]\n",
      "loss: 0.580324  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.498611 \n",
      "\n",
      "Epoch 13\n",
      "----------------------------\n",
      "loss: 0.440258  [    0/  192]\n",
      "loss: 0.388172  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.455964 \n",
      "\n",
      "Epoch 14\n",
      "----------------------------\n",
      "loss: 0.457438  [    0/  192]\n",
      "loss: 0.453551  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.449051 \n",
      "\n",
      "Epoch 15\n",
      "----------------------------\n",
      "loss: 0.535180  [    0/  192]\n",
      "loss: 0.472471  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.414803 \n",
      "\n",
      "Epoch 16\n",
      "----------------------------\n",
      "loss: 0.378595  [    0/  192]\n",
      "loss: 0.359975  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.431164 \n",
      "\n",
      "Epoch 17\n",
      "----------------------------\n",
      "loss: 0.425745  [    0/  192]\n",
      "loss: 0.435609  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.442995 \n",
      "\n",
      "Epoch 18\n",
      "----------------------------\n",
      "loss: 0.387055  [    0/  192]\n",
      "loss: 0.411087  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.457649 \n",
      "\n",
      "Epoch 19\n",
      "----------------------------\n",
      "loss: 0.404961  [    0/  192]\n",
      "loss: 0.496646  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.390696 \n",
      "\n",
      "Epoch 20\n",
      "----------------------------\n",
      "loss: 0.511290  [    0/  192]\n",
      "loss: 0.309141  [  100/  192]\n",
      "Test Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.377259 \n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold=KFold(n_splits=3,shuffle=True)\n",
    "\n",
    "for fold,(train_idx,test_idx) in enumerate(kfold.split(x_train[0,0,:])):\n",
    "    print('------------fold no---------{}----------------------'.format(fold))\n",
    "  \n",
    "    # train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    # test_subsampler = torch.utils.data.SubsetRandomSampler(test_idx)\n",
    "    X = x_train[:,:,train_idx]\n",
    "    Y = y_train[train_idx]\n",
    "    x_val = x_train[:,:,test_idx]\n",
    "    y_val = y_train[test_idx]\n",
    "    # print(X.shape)\n",
    "    # print(Y.shape)\n",
    "    # print(x_val.shape)\n",
    "    # print(y_val.shape)\n",
    "    # print(train_idx)\n",
    "    # print(test_idx)\n",
    "    \n",
    "    train_dataloader = DataLoader(CustomDataset(X , Y), batch_size = batch_size, shuffle=True)\n",
    "    val_dataloader  = DataLoader(CustomDataset(x_val , y_val), batch_size = batch_size, shuffle=True)\n",
    "    \n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    model.apply(reset_weights)\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        \n",
    "        print(f\"Epoch {t+1}\\n----------------------------\")\n",
    "        train_loop(train_dataloader,model,loss_fn,optimizer)\n",
    "        test_loop(val_dataloader,model,loss_fn)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.88541666666666"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataloader, model):\n",
    "    model.eval()\n",
    "    pred_l = []\n",
    "    with torch.no_grad():\n",
    "        for X, _ in dataloader:\n",
    "            pred = model(X)\n",
    "            pred_l.append(torch.round(pred).item())\n",
    "    return np.array(pred_l,dtype=uint8)\n",
    "\n",
    "test_loader  = DataLoader(CustomDataset(x_test , y_test))\n",
    "\n",
    "pred = predict(test_loader,model)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix'):\n",
    "    \"\"\"\n",
    "    \n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    ax = sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "\n",
    "    ax.set_xlabel('\\nPredicted Values')\n",
    "    ax.set_ylabel('Actual Values ');\n",
    "\n",
    "    ## Ticket labels - List must be in alphabetical order\n",
    "    ax.xaxis.set_ticklabels(classes)\n",
    "    ax.yaxis.set_ticklabels(classes)\n",
    "\n",
    "    ## Display the visualization of the Confusion Matrix.\n",
    "    plt.show()\n",
    "\n",
    "def accuray_report(pred,true,report=True):\n",
    "    if report:\n",
    "      print(10*'*')\n",
    "      print('Confusion Matrix')\n",
    "      print(10*'*')\n",
    "      cm = confusion_matrix(true , pred)\n",
    "      print(cm)\n",
    "      plot_confusion_matrix(cm,['Low','Medium'])\n",
    "      \n",
    "      print(10*'*')\n",
    "      print('Classification Report')\n",
    "      print(10*'*')\n",
    "      print(classification_report(true , pred ))\n",
    "    return accuracy_score(true,pred)\n",
    "\n",
    "def save_plot_graphs(experiment_name , train_acc,val_acc,train_losses,val_losses):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(15,5))\n",
    "    ax1.plot(train_losses, label='Training loss')\n",
    "    ax1.plot(val_losses, label='Validation loss')\n",
    "    # ax1.xlabel('Epochs')\n",
    "    # ax1.ylabel('Loss Value')\n",
    "    ax1.set(xlabel='Epochs', ylabel='Loss Value')\n",
    "    ax1.legend()\n",
    "    \n",
    "    \n",
    "    ax2.plot(train_acc, label='Training Accuracy')\n",
    "    ax2.plot(val_acc, label='Validation Accuracy')\n",
    "    # ax2.xlabel('Epochs')\n",
    "    # ax2.ylabel('Accuracy')\n",
    "    ax2.set(xlabel='Epochs', ylabel='Loss Value')\n",
    "    ax2.legend()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Confusion Matrix\n",
      "**********\n",
      "[[31  5]\n",
      " [ 2 34]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Classification Report\n",
      "**********\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.86      0.90        36\n",
      "           1       0.87      0.94      0.91        36\n",
      "\n",
      "    accuracy                           0.90        72\n",
      "   macro avg       0.91      0.90      0.90        72\n",
      "weighted avg       0.91      0.90      0.90        72\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9027777777777778"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuray_report(pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_plot_graphs('experiment_name' , train_acc,val_acc,train_losses,val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f73ca796a9fc520160c68fb082adeee8cca72dd22cecda376fb3fa4e6ba6f920"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
